# Module for optimizing classifier through iterative update of conditional probabilities. 
from typing import List
from bayes import BayesClassifier, calcClassProb
from space import MeasurementGenerator, ClassAssign
from testing import shuffle, vFold
from evaluation import calcExpGain

def train(tagger: BayesClassifier, iter: int, measGen: MeasurementGenerator, classGen: ClassAssign, Z: int, V: int=10, delta: float=.05)-> List[float]:
		# Performs 'iter' iterations of vFold testing (default 'V' is ten) with 'tagger' classifier
		# for 'Z' samples generated by 'measGen' and 'classGen.' After each vFold validation, appends
		# an expected value (attached to tagger) and then optimizes tagger by 'delta' paramenter (default .05).
		# Outputs a new optimized tagger and list of gain values from each iteration.
		expectedGain = []

		for _ in range(iter):
			# Generates measurements
			samples = measGen.genMeas(Z)
			values = classGen.assign(samples)

			# Shuffles values
			samplesSh, valuesSh = shuffle(samples,values)

			# Performs Test
			matrix = vFold(samplesSh, valuesSh, V, tagger)
			# Appends value to list
			expectedGain.append(calcExpGain(matrix, tagger.eGain))

			# Gives class probability over whole data set.
			tagger.priorUpdate(calcClassProb(valuesSh, tagger.range))

			# Updates tagger
			tagger.optimize(delta, samplesSh, valuesSh)

		return expectedGain

def main():
	from random import seed
	from probability import genProbs
#### Unit test for training funciton
	# Initiatializes seed for recurrent testing.
	for _ in range(10):
		dimen = (4,3,6,5,6)
		classValues = 5

		measures = MeasurementGenerator(dimen)
		classes = ClassAssign(dimen, classValues)

		conds = [list(genProbs(measures.range)) for _ in range(classValues)]
		egain = [[2,0,0,0,1],[3,4,0,2,2],[2,2,5,1,1],[2,2,3,4,1],[0,1,-3,2,3]]
		classifier = BayesClassifier(None, conds, eGain=egain) # Worries that supplying similar priors is affecting our results. Even though vFold updates.
		y = train(classifier, 20, measures, classes, 6000, delta=.0005)
		z = [y[i] - y[i-1] for i in range(1, len(y))]
		# Trying to figure out average negative error to see if this is floating point. 
		print(y)
		print()
		print(z)
		q = [i for i in z if i < 0]
		q = sum(q)/max(len(q), 1)
		print(q)
		print()
	x = measures.genMeas(20)

	p = classes.assign(x)
	l = classifier.assign(x)



if __name__ == "__main__":
    main()